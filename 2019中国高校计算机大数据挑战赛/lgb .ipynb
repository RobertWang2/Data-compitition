{"cells":[{"outputs":[{"output_type":"stream","text":"Requirement already satisfied: fuzzywuzzy in /opt/conda/lib/python3.6/site-packages\ntest.csv  train.csv\n","name":"stdout"}],"source":"import pandas as pd\n# import  modin.pandas as pd\nimport numpy as np\nimport re\nfrom bs4 import BeautifulSoup\nfrom gensim.models import word2vec\npath = '/home/kesci/input/bytedance/first-round/'\nimport re  # For preprocessing\nimport pandas as pd  # For data handling\nfrom time import time  # To time our operations\nfrom collections import defaultdict  # For word frequency\nimport time\nfrom tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n#tfidf\nfrom sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer\nfrom scipy.sparse import coo_matrix, vstack,hstack\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom gensim.models import KeyedVectors\nimport spacy  # For preprocessing\nimport multiprocessing\nfrom gensim.models import Word2Vec\nfrom gensim.models.word2vec import LineSentence\nimport gc \nfrom gensim.models.phrases import Phrases, Phraser\nfrom gensim import corpora\nfrom gensim.models import LdaModel\nfrom gensim.corpora import Dictionary\nfrom gensim import corpora\nimport Levenshtein\n#线性代数中的范数\nfrom scipy.linalg import norm\nfrom sklearn.datasets import load_boston\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import  make_classification\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import RidgeCV, LassoCV,Ridge, Lasso\nfrom sklearn.feature_selection import SelectKBest,chi2\nfrom sklearn.metrics.pairwise import euclidean_distances,manhattan_distances,cosine_distances\n!pip install  --index \"http://pypi/simple\" --trusted-host pypi  fuzzywuzzy\nfrom fuzzywuzzy import fuzz\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n!ls /home/kesci/input/bytedance/first-round/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"DA39D1D8207642848CA54CA86AC0088C","scrolled":false},"execution_count":2},{"outputs":[],"execution_count":3,"source":"#读数据\r\ntrain = pd.read_csv('lgb_train.csv'\r\n    )\r\n# train = tra_data.get_chunk(5000000)\r\n\r\ntest = pd.read_csv('lgb_test.csv'\r\n    )","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"6CD61B511D0B42EB91FA191AC269DAD9","scrolled":false}},{"metadata":{"id":"A7C4B0FC3CF74BAD884F07E8D17612B6","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"#Build doc\n\ndef train_word2vec(init = False):\n    global test,train\n    if init == True:\n        title = [s.split() for s in train['title'].values]\n        query = [s.split() for s in train['query'].values]\n        doc = title + query\n        title = [s.split() for s in test['title'].values]\n        query = [s.split() for s in test['query'].values]\n        doc += title\n        doc += query\n        gc.collect()\n        # del test,title,query\n        #build vocabulary\n        import gensim\n        model = gensim.models.Word2Vec(size=128, window=3, min_count=1, sg=1, workers=10)\n        model.build_vocab(doc)  # prepare the model vocabulary\n        model.train(sentences=doc, total_examples=model.corpus_count, epochs=model.iter)\n        model.save('word2vec.model')\n        #model = Word2Vec.load(\"word2vec.model\")\n        del doc\ntrain_word2vec(True)","execution_count":5},{"metadata":{"id":"A7EE604F3832445AAD50EEF952B39F79","mdEditEnable":false},"cell_type":"markdown","source":"Feature Engineering"},{"metadata":{"id":"56182838C9B442218D724EF265EE4BE2","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def Leven_distance_list(row): \r\n    '''\r\n    #编辑距离 脱敏后的词语\r\n    '''\r\n    str1 = row['query'].split()\r\n    str2 = row['title'].split()\r\n    dp=np.zeros((len(str1)+1,len(str2)+1))\r\n    m=len(str1)\r\n    n=len(str2)\r\n    for k in range(1,m+1):\r\n         dp[k][0]=k\r\n    for k in range(1,n+1):\r\n         dp[0][k]=k\r\n    for k in range(1,m+1):\r\n         for j in range(1,n+1):\r\n            dp[k][j]=min(dp[k-1][j],dp[k][j-1])+1 #这里表示上边和下边的数值最小数值\r\n            if str1[k-1]==str2[j-1]:\r\n                dp[k][j]=min(dp[k][j],dp[k-1][j-1])\r\n            else:\r\n                dp[k][j]=min(dp[k][j],dp[k-1][j-1]+1)\r\n    return int(dp[-1][-1])\r\ndef seqratio(row):\r\n    '''\r\n    基于编辑距离的相似度\r\n    '''\r\n    edit_distance = Leven_distance_list(row)\r\n    len1 = len(row['query'].split())\r\n    len2 = len(row['title'].split())\r\n    return (len1 + len2 - edit_distance) / (len1 + len2)\r\n#train.head().apply(seqratio,axis=1)\r\ndef jaccard_similarity(row):\r\n    '''\r\n    基于杰卡迪的相似度\r\n    '''\r\n    str1 = row['query']\r\n    str2 = row['title']\r\n    cv = CountVectorizer(tokenizer= lambda s : s.split())\r\n    corpus = [str1,str2]\r\n    vec = cv.fit_transform(corpus).toarray()\r\n    inter = np.sum(np.min(vec,axis=0))\r\n    merge = np.sum(np.max(vec,axis=0))\r\n    return 1.0* inter/merge\r\n\r\ndef tf_similarity(row):\r\n    '''\r\n    基于tf的余弦相似度特征\r\n    '''\r\n    s1 = row['title']\r\n    s2 = row['query']\r\n    cv = CountVectorizer(tokenizer=lambda x : x.split())\r\n    corpus = [s1,s2]\r\n    vector = cv.fit_transform(corpus).toarray()\r\n    return np.dot(vector[0],vector[1]) / (norm(vector[0])*norm(vector[1]))\r\n#     return cosine_similarity(vector)[1][0]\r\n\r\ndef tfidf_similarity(row):\r\n    '''\r\n    基于tf-idf的余弦相似度特征\r\n    '''\r\n    s1 = row['title']\r\n    s2 = row['query']\r\n    tfidf = TfidfVectorizer(tokenizer=lambda x :x.split())\r\n    corpus = [s1,s2]\r\n    vector = tfidf.fit_transform(corpus).toarray()\r\n    return np.dot(vector[0],vector[1]) / (norm(vector[0])*norm(vector[1]))\r\ndef lda(df):\r\n     '''\r\n     主题模型\r\n     '''\r\n     query = df['query'].values\r\n     query = list(map(lambda x : x.split(' '),query))\r\n     dictionary = corpora.Dictionary(query)\r\n     corpus = [dictionary.doc2bow(words) for words in query]\r\n     lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=50)\r\n     vec = lda.inference(corpus)\r\n     res = np.argmax(vec[0],axis=1)\r\n     df['lda'] = res.reshape(-1,1)\r\n     return df\r\ndef Levenshtein_dis_rate(item):\r\n    '''\r\n    计算prefix和title的距离比例,即距离/长度\r\n    '''\r\n    str1=item['query'].split(' ')\r\n    str2=item['title'].split(' ')\r\n    leven_distance=item['distance']\r\n    length=max(len(str1),len(str2))\r\n    return leven_distance/length   \r\n    \r\ndef LCS(item):\r\n    '''\r\n    计算str1和str2之间的最长公共子串的长度\r\n    '''\r\n    str1=item['query'].split(' ')\r\n    str2=item['title'].split(' ')\r\n    if len(str1)==0 or len(str2) ==0:\r\n         return 0\r\n    len1 = len(str1)\r\n    len2 = len(str2)\r\n\r\n    c = np.zeros((len2+1,),dtype=np.int32)\r\n    max_len = 0\r\n    for i in range(len1):\r\n        for j in range(len2-1,-1,-1):\r\n            if str1[i] == str2[j]:\r\n                c[j+1] = c[j] + 1\r\n                if c[j+1]>=max_len:\r\n                    max_len = c[j+1]\r\n            else:\r\n                c[j+1] = 0\r\n    return max_len\r\ndef rp(df):\r\n    '''\r\n    recall precision acc\r\n    '''\r\n    def rp_query_in_title(item): #计算acc，recall，precisoin\r\n        query=item['query'].split(' ')\r\n        title=item['title'].split(' ')\r\n        len_title=len(title)\r\n        len_query=len(query)\r\n        len_comm_xx=len(set(query)&set(title))\r\n        recall=len_comm_xx/(len_query+0.01)\r\n        precision=len_comm_xx/(len_title+0.01)\r\n        acc=len_comm_xx/(len_title+len_query-len_comm_xx+0.01)\r\n        return [recall,precision,acc]\r\n    \r\n    a = df.apply(rp_query_in_title,axis=1)\r\n    a = np.array(a.values.tolist())\r\n    # print(a.shape)\r\n    df['recall'] = a[:,0]\r\n    df['precision'] = a[:,1]\r\n    df['acc'] = a[:,2]\r\n    return df  \r\ndef w2v_simi(df):\r\n    '''\r\n    word2vec相似度特征\r\n    '''\r\n    w2v = Word2Vec.load(\"word2vec.model\")\r\n    def w2v_similarity(row):\r\n        s1 = row['query']\r\n        s2 = row['title']\r\n        def sentence_vector(s):\r\n            words = s.split()\r\n            v = np.zeros(128)\r\n            for i in words:\r\n                v+=w2v[i]\r\n            v/=len(s)\r\n            return v\r\n        v1,v2 = sentence_vector(s1),sentence_vector(s2)\r\n        return np.dot(v1,v2) / (norm(v1)*norm(v2))\r\n    return df.apply(w2v_similarity,axis=1)\r\n\r\ndef query_complete_in_title(item):\r\n    '''\r\n    是否都在title里\r\n    '''\r\n    query=item['query'].split(' ')\r\n    title=item['title'].split(' ')\r\n    n=0\r\n    for word in query:\r\n        if word in title:\r\n            n+=1\r\n    return 1 if n==len(query) else 0\r\n    \r\ndef title_complete_in_query(item):\r\n    '''\r\n    是否都在query里\r\n    '''\r\n    query=item['query'].split(' ')\r\n    title=item['title'].split(' ')\r\n    n=0\r\n    for word in title:\r\n        if word in query:\r\n            n+=1\r\n    return 1 if n==len(title) else 0\r\ndef query_and_title(item):\r\n    '''\r\n    query 与 title 交集\r\n    '''\r\n    query=item['query'].split(' ')\r\n    title=item['title'].split(' ')  \r\n    num=0\r\n    for word in query:\r\n        if word in title:\r\n            num+=1\r\n    return num\r\ndef query_in_title_of_query(item):\r\n    '''\r\n    query 与 title交集/query\r\n    '''\r\n    query=item['query'].split(' ')\r\n    title=item['title'].split(' ')  \r\n    num=0\r\n    if title!=' ':\r\n        for word in query:\r\n            if word in title:\r\n                num+=1\r\n        return num/len(query)\r\n    return 0\r\ndef query_in_title_of_title(item):\r\n    '''\r\n    query 与 title交集/ title\r\n    '''\r\n    query=item['query'].split(' ')\r\n    title=item['title'].split(' ')  \r\n    num=0\r\n    if title!=' ':\r\n        for word in query:\r\n            if word in title:\r\n                num+=1\r\n        return num/len(title)\r\n    return 0\r\ndef fuzz_ratio(row):\r\n    '''\r\n    Simple Ratio\r\n    '''\r\n    s1 = row['query']\r\n    s2 = row['title']\r\n    return fuzz.ratio(s1,s2)\r\n    \r\ndef partial_ratio(row):\r\n    '''\r\n    Partial  Ratio\r\n    '''\r\n    s1 = row['query']\r\n    s2 = row['title']\r\n    return fuzz.partial_ratio(s1,s2)\r\n    \r\ndef token_sort_ratio(row):\r\n    '''\r\n    token_sort_ratio\r\n    '''\r\n    s1 = row['query']\r\n    s2 = row['title']\r\n    return fuzz.token_sort_ratio(s1,s2)\r\n    \r\ndef token_set_ratio(row):\r\n    '''\r\n    token_set_ratio\r\n    '''\r\n    s1 = row['query']\r\n    s2 = row['title']\r\n    return fuzz.token_set_ratio(s1,s2)\r\n    \r\ndef Word_Share(df):\r\n    '''\r\n    Word_Share\r\n    '''\r\n    def get_weight(count, eps=10000, min_count=2):\r\n        return 0 if count < min_count else 1 / (count + eps)\r\n\r\n    # title = [s.split() for s in df['title'].values]\r\n    # query = [s.split() for s in df['query'].values]\r\n    # words = title + query\r\n    train_qs = pd.Series(df['query'].tolist() + df['title'].tolist()).astype(str)\r\n    words = (\" \".join(df)).lower().split()\r\n    counts = Counter(words)\r\n    weights = {word: get_weight(count) for word, count in counts.items()}\r\n    \r\n    # stops = set(stopwords.words(\"english\"))\r\n    def word_shares(row):\r\n    \tq1words = set(str(row['query']).split())\r\n    \tq2words = set(str(row['title']).split())\r\n    \r\n    \tshared_words = q1words.intersection(q2words)\r\n    \tshared_weights = [weights.get(w, 0) for w in shared_words]\r\n    # \ttotal_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\r\n    \t\r\n    # \tR1 = np.sum(shared_weights) / (np.sum(total_weights)+10000) #tfidf share\r\n    # \tprint(f'np.sum(shared_weights) = {np.sum(shared_weights)}, total_weights = {total_weights}')\r\n    \tR2 = len(shared_words) / (len(q1words) + len(q2words)) #count share\r\n    \treturn '{}:{}'.format( R2, len(shared_words))\r\n        \r\n        # Concat train and test datasets\r\n    # df = pd.concat([train, test])\r\n    df['word_shares'] = df.apply(word_shares, axis=1, raw=True)\r\n    df['tfidf_word_match'] = df['word_shares'].apply(lambda x: float(x.split(':')[0]))\r\n    df['shared_count']     = df['word_shares'].apply(lambda x: float(x.split(':')[1]))\r\n    return df.drop('word_shares',axis=1)\r\n\r\ndef num_of_query_each_title(df):\r\n    '''\r\n    query_id对应的个数\r\n    '''\r\n    num_list=[]\r\n    query_dict={}\r\n    for i in tqdm(range(len(df))):\r\n        if df.iloc[i]['query_id'] not in query_dict:\r\n            num=len(df[df['query_id']==df.iloc[i]['query_id']])\r\n            query_dict[df.iloc[i]['query_id']]=num\r\n            num_list.append(num)\r\n        else:\r\n            num_list.append(query_dict[df.iloc[i]['query_id']])\r\n    return num_list\r\ndef Title_of_query(df):\r\n    '''\r\n    Title_of_query\r\n    '''\r\n    def title_of_query(df):\r\n        start=time.time()\r\n        result=[]\r\n        for i in df.groupby(['query_id'])['query_id']:\r\n            temp=df[df['query_id']==i[0]]['title'].values\r\n            temp=list(map(lambda x:x.split(' '),temp))\r\n            max_len=0\r\n            sum_len=0\r\n            min_len=99999\r\n            for line in temp:\r\n                l=len(line)\r\n                max_len=max(max_len,l)\r\n                min_len=min(min_len,l)\r\n                sum_len+=l\r\n            average_len=sum_len/len(temp)\r\n            for j in range(len(i[1])):\r\n                result.append([max_len,min_len,average_len])\r\n        print('用时:',time.time()-start)\r\n        return result\r\n    a = title_of_query(df)\r\n    a = np.array(a)\r\n    # print(a.shape)\r\n    df['max_len'] = a[:,0]\r\n    df['min_len'] = a[:,1]\r\n    df['average_len'] = a[:,2]\r\n    return df\r\n  \r\nfrom sklearn.metrics.pairwise import manhattan_distances\r\nfrom sklearn.metrics.pairwise import haversine_distances\r\ndef Euclidean_distances(df):\r\n    '''\r\n    euclidean_distances\r\n    '''\r\n    w2v = Word2Vec.load(\"word2vec.model\")\r\n    def sentence_vector(row):\r\n        s1 = row['query']\r\n        s2 = row['title']\r\n        def sentence_vector(s):\r\n            words = s.split()\r\n            v = np.zeros(128)\r\n            for i in words:\r\n                v+=w2v[i]\r\n            v/=len(s)\r\n            return v\r\n        v1,v2 = sentence_vector(s1),sentence_vector(s2)\r\n        return  euclidean_distances([v1],[v2])[0][0]\r\n    return df.apply(sentence_vector,axis=1)\r\n\r\n\r\ndef Manhattan_distances(df):\r\n    '''\r\n    euclidean_distances\r\n    '''\r\n    w2v = Word2Vec.load(\"word2vec.model\")\r\n    def sentence_vector(row):\r\n        s1 = row['query']\r\n        s2 = row['title']\r\n        def sentence_vector(s):\r\n            words = s.split()\r\n            v = np.zeros(128)\r\n            for i in words:\r\n                v+=w2v[i]\r\n            v/=len(s)\r\n            return v\r\n        v1,v2 = sentence_vector(s1),sentence_vector(s2)\r\n        return manhattan_distances([v1],[v2])[0][0]\r\n    return df.apply(sentence_vector,axis=1)\r\n\r\n\r\ndef Cosine_distances(df):\r\n    '''\r\n    euclidean_distances\r\n    '''\r\n    w2v = Word2Vec.load(\"word2vec.model\")\r\n    def sentence_vector(row):\r\n        s1 = row['query']\r\n        s2 = row['title']\r\n        def sentence_vector(s):\r\n            words = s.split()\r\n            v = np.zeros(128)\r\n            for i in words:\r\n                v+=w2v[i]\r\n            v/=len(s)\r\n            return v\r\n        v1,v2 = sentence_vector(s1),sentence_vector(s2)\r\n        return cosine_distances([v1],[v2])[0][0]\r\n    return df.apply(sentence_vector,axis=1)\r\n\r\n\r\n\r\n#w2v d2v相似度 其他..\r\n","execution_count":4},{"metadata":{"id":"00BC9601A7064DF18393AA0119DF53D5","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"#Basic statistical features sometimes make sense in model accuracy. We can call them meta features.\ndef feature_extraction(data):\n    \n    #统计特征\n    \n    #1. Number of query words \n    data['query_num'] = data['query'].apply(lambda x : len(str(x).split()))\n    #2. Number of title words \n    data['title_num'] = data['title'].apply(lambda x : len(str(x).split()))\n    #3. diffence between query num and title num\n    data['diff_len_word'] = np.abs(data['query_num'] - data['title_num'])\n    #4. query length\n    data['len_q1'] = data['query'].apply(lambda x: len(str(x)))\n    #5. title length\n    data['len_q2'] = data['title'].apply(lambda x: len(str(x)))\n    #6. diffence between query length and title\n    data['diff_len'] = np.abs(data['len_q2'] - data['len_q1'])\n    #7. query char length\n    data['len_char_q1'] = data['query'].apply(lambda x: len(str(x).replace(' ', '')))\n    #8. title char length\n    data['len_char_q2'] = data['title'].apply(lambda x: len(str(x).replace(' ', '')))\n    #9. diffence between query char length and title char length\n    data['diff_len_char'] = np.abs(data['len_char_q1'] - data['len_char_q2'])\n    #10. avg_world_query\n    data['avg_world_len1'] = data['len_char_q1'] / data['query_num']\n    #11. avg_world_title\n    data['avg_world_len2'] = data['len_char_q2'] / data['title_num']\n    #12. diffence between avg_world_query and avg_world_title\n    data['diff_avg_word'] = np.abs(data['avg_world_len1'] - data['avg_world_len2'])\n    #13. common_words\n    data['common_words'] = data.apply(lambda x: len(set(str(x['query']).split()).intersection(set(str(x['title']).split()))),axis=1)\n    ##14. Number of query unique words in the text ##\n    data['query_unique_num'] = data['query'].apply(lambda x : len(set(str(x).split())))\n    ##15. Number of title unique words in the text ##\n    data['title_unique_num'] = data['title'].apply(lambda x : len(set(str(x).split())))\n    ##16. Average length of the query words in the text ##\n    data['mean_query_len'] = data['query'].apply(lambda x : np.mean([len(w) for w in str(x).split()]))\n    ##17. Average length of the title words in the text ##\n    data['mean_title_len'] = data['title'].apply(lambda x : np.mean([len(w) for w in str(x).split()]))\n    #18. 19. 21. Title_of_query  max min avg\n    # data = Title_of_query(data)\n    #21. num_of_query_each_title\n    # data['num_of_query_each_title'] = num_of_query_each_title(data)\n    ## 相似度特征\n    \n    #18. Edit distance\n    data['distance'] = data.apply(Leven_distance_list,axis=1)\n    #19. edit distance ratio\n    data['seqratio'] = data.apply(seqratio,axis=1)\n    #20. jaccard_similarity\n    data['jaccard_similarity'] = data.apply(jaccard_similarity,axis=1)\n    #21. tfidf_similarity\n    data['tfidf_similarity'] = data.apply(tfidf_similarity,axis=1)\n    #22. word2vec similarity\n    data['w2v_simi'] = w2v_simi(data)\n    #23. tf_similarity\n    data['tf_similarity'] = data.apply(tf_similarity,axis=1)\n    #24.Simple Ratio\n    data['fuzz_ratio'] = data.apply(fuzz_ratio,axis=1)\n    #25. Partial Ratio\n    data['partial_ratio'] = data.apply(partial_ratio,axis=1)\n    #27. Token Sort Ratio\n    data['token_sort_ratio'] = data.apply(token_sort_ratio,axis=1)\n    #28. Token Set Ratio\n    data['token_set_ratio'] = data.apply(token_set_ratio,axis=1)\n    \n    #其他特征\n    \n    #29. Levenshtein_dis_rate\n    data['Levenshtein_dis_rate'] = data.apply(Levenshtein_dis_rate,axis=1)\n    #30. 最长上升子序列\n    data['LCS'] = data.apply(LCS,axis=1)\n    #31/ LDA 提取主题后，找到最大主题词的id\n    data = lda(data)\n    #32. 是否都在title里面\n    data['query_complete_in_title'] = data.apply(query_complete_in_title,axis=1)\n    #33. 是否都在query里面\n    data['title_complete_in_query'] = data.apply(title_complete_in_query,axis=1)\n    #34. query 与 title 交集\n    data['query_and_title'] = data.apply(query_and_title,axis=1)\n    #35. query 与 title交集/query\n    data['query_in_title_of_query'] = data.apply(query_in_title_of_query,axis=1)\n    #36. query 与 title交集/title\n    data['query_in_title_of_title'] = data.apply(query_in_title_of_title,axis=1)\n    #37. recall\n    #38. precision\n    #39. acc\n    data = rp(data)\n    #40. tfidf_word_match\n    #41. shared_count\n    data = Word_Share(data)\n    #42. num_of_query_each_title\n    # data['num_of_query_each_title'] = num_of_query_each_title(data)\n    #43.max\n    #44.min\n    #45.avg\n    data = Title_of_query(data)\n    #46.euclidean_distances\n    data['euclidean_distances'] = Euclidean_distances(data)\n    #47.manhattan_distances\n    data['manhattan_distances'] = Manhattan_distances(data)\n    #48.cosine_distances\n    data['cosine_distances'] = Cosine_distances(train)\n    print(data.head()) \n    \n    return data\ntrain = feature_extraction(train)\n#保存处理好的文件\ntrain.to_csv('lgb_train.csv',index=False)\ntest = feature_extraction(test)\ntest.to_csv('lgb_test.csv',index=False)","execution_count":5},{"metadata":{"id":"F421C85D47064A049A0D54576A5F3D76","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"#划分的用于普通lgb，train用于交叉验证lgb\n#X_train,X_test, y_train, y_test = train_test_split(train,train['label'],test_size=0.3, random_state=0)\nX_train,X_test, y_train, y_test = train_test_split(train.drop('label',axis=1),train['label'],test_size=0.3, random_state=0)\n# X_train,X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=0)\n\n","execution_count":5},{"metadata":{"id":"7FC3A57464C24596AF6655C18EBE8E45","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":2},{"metadata":{"id":"F45915CECA2641DDACFF43073402BC40","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"Starting training ...\n\ny_tr distribution: Counter({0: 5958083, 1: 2041917})\ntraining LGB:\n[1]\ttraining's binary_logloss: 0.562131\tvalid_1's binary_logloss: 0.562674\nTraining until validation scores don't improve for 5 rounds.\n[2]\ttraining's binary_logloss: 0.558207\tvalid_1's binary_logloss: 0.558745\n[3]\ttraining's binary_logloss: 0.555457\tvalid_1's binary_logloss: 0.556009\n[4]\ttraining's binary_logloss: 0.553539\tvalid_1's binary_logloss: 0.554101\n[5]\ttraining's binary_logloss: 0.552184\tvalid_1's binary_logloss: 0.552761\n[6]\ttraining's binary_logloss: 0.551148\tvalid_1's binary_logloss: 0.551731\n[7]\ttraining's binary_logloss: 0.550382\tvalid_1's binary_logloss: 0.550955\n[8]\ttraining's binary_logloss: 0.549701\tvalid_1's binary_logloss: 0.550268\n[9]\ttraining's binary_logloss: 0.549222\tvalid_1's binary_logloss: 0.549803\n[10]\ttraining's binary_logloss: 0.548741\tvalid_1's binary_logloss: 0.549332\n[11]\ttraining's binary_logloss: 0.548374\tvalid_1's binary_logloss: 0.548976\n[12]\ttraining's binary_logloss: 0.548105\tvalid_1's binary_logloss: 0.548716\n[13]\ttraining's binary_logloss: 0.547723\tvalid_1's binary_logloss: 0.54833\n[14]\ttraining's binary_logloss: 0.547351\tvalid_1's binary_logloss: 0.547958\n[15]\ttraining's binary_logloss: 0.547159\tvalid_1's binary_logloss: 0.547769\n[16]\ttraining's binary_logloss: 0.546955\tvalid_1's binary_logloss: 0.547566\n[17]\ttraining's binary_logloss: 0.546783\tvalid_1's binary_logloss: 0.547392\n[18]\ttraining's binary_logloss: 0.546364\tvalid_1's binary_logloss: 0.546968\n[19]\ttraining's binary_logloss: 0.546242\tvalid_1's binary_logloss: 0.546854\n[20]\ttraining's binary_logloss: 0.546008\tvalid_1's binary_logloss: 0.54663\n[21]\ttraining's binary_logloss: 0.5459\tvalid_1's binary_logloss: 0.54653\n[22]\ttraining's binary_logloss: 0.545807\tvalid_1's binary_logloss: 0.546445\n[23]\ttraining's binary_logloss: 0.545413\tvalid_1's binary_logloss: 0.546048\n[24]\ttraining's binary_logloss: 0.545338\tvalid_1's binary_logloss: 0.545981\n[25]\ttraining's binary_logloss: 0.545085\tvalid_1's binary_logloss: 0.545735\n[26]\ttraining's binary_logloss: 0.544945\tvalid_1's binary_logloss: 0.545602\n[27]\ttraining's binary_logloss: 0.544889\tvalid_1's binary_logloss: 0.545575\n[28]\ttraining's binary_logloss: 0.544824\tvalid_1's binary_logloss: 0.545527\n[29]\ttraining's binary_logloss: 0.544763\tvalid_1's binary_logloss: 0.545473\n[30]\ttraining's binary_logloss: 0.544487\tvalid_1's binary_logloss: 0.545206\n[31]\ttraining's binary_logloss: 0.544437\tvalid_1's binary_logloss: 0.545162\n[32]\ttraining's binary_logloss: 0.544287\tvalid_1's binary_logloss: 0.545016\n[33]\ttraining's binary_logloss: 0.544214\tvalid_1's binary_logloss: 0.544953\n[34]\ttraining's binary_logloss: 0.544037\tvalid_1's binary_logloss: 0.544779\n[35]\ttraining's binary_logloss: 0.543939\tvalid_1's binary_logloss: 0.544683\n[36]\ttraining's binary_logloss: 0.543874\tvalid_1's binary_logloss: 0.544622\n[37]\ttraining's binary_logloss: 0.543792\tvalid_1's binary_logloss: 0.544531\n[38]\ttraining's binary_logloss: 0.543725\tvalid_1's binary_logloss: 0.544466\n[39]\ttraining's binary_logloss: 0.543676\tvalid_1's binary_logloss: 0.54442\n[40]\ttraining's binary_logloss: 0.543628\tvalid_1's binary_logloss: 0.544377\n[41]\ttraining's binary_logloss: 0.543435\tvalid_1's binary_logloss: 0.544192\n[42]\ttraining's binary_logloss: 0.543406\tvalid_1's binary_logloss: 0.54417\n[43]\ttraining's binary_logloss: 0.543364\tvalid_1's binary_logloss: 0.544136\n[44]\ttraining's binary_logloss: 0.54332\tvalid_1's binary_logloss: 0.544103\n[45]\ttraining's binary_logloss: 0.543283\tvalid_1's binary_logloss: 0.544072\n[46]\ttraining's binary_logloss: 0.543243\tvalid_1's binary_logloss: 0.544038\n[47]\ttraining's binary_logloss: 0.543104\tvalid_1's binary_logloss: 0.543889\n[48]\ttraining's binary_logloss: 0.543063\tvalid_1's binary_logloss: 0.543839\n[49]\ttraining's binary_logloss: 0.543019\tvalid_1's binary_logloss: 0.543799\n[50]\ttraining's binary_logloss: 0.542906\tvalid_1's binary_logloss: 0.543684\nDid not meet early stopping. Best iteration is:\n[50]\ttraining's binary_logloss: 0.542906\tvalid_1's binary_logloss: 0.543684\n\ny_tr distribution: Counter({0: 5957335, 1: 2042665})\ntraining LGB:\n","name":"stdout"}],"source":"#交叉验证的LGB\r\nfrom sklearn.model_selection import KFold\r\n# model = model.Booster(model_file='lgb.model')\r\nparams = {\r\n    'task': 'train',\r\n    'boosting_type': 'gbdt',  # 设置提升类型\r\n    'objective': 'regression', # 目标函数\r\n    'metric': {'binary_logloss'},  # 评估函数\r\n    # 'num_leaves': 32,   # 叶子节点数\r\n    # 'max_depth':5,   #最大数的深度\r\n    'learning_rate': 0.2,  # 学习速率0.05 0.1\r\n    # 'feature_fraction': 0.9, # 建树的特征选择比例\r\n    # 'bagging_fraction': 0.9, # 建树的样本采样比例\r\n    # 'bagging_freq': 5,  # k 意味着每 k 次迭代执行bagging\r\n    #  'lambda': 5,\r\n    'verbose': 1 # <0 显示致命的, =0 显示错误 (警告), >0 显示信息\r\n}\r\nn_splits = 5\r\noof_train = np.zeros((train.shape[0]))\r\noof_test = np.zeros((test.shape[0], n_splits))\r\ntrain = train.drop(['query','title','query_id','query_title_id'],axis=1)\r\n\r\nkfold = KFold(n_splits = n_splits, random_state=1337)\r\ni = 0\r\nprint('Starting training ...')\r\nfor train_index, valid_index in kfold.split(train, train['label'].values):\r\n    # print(train_index)\r\n    X_tr = train.iloc[train_index, :]\r\n    X_val = train.iloc[valid_index, :]\r\n    \r\n    y_tr = X_tr['label'].values\r\n    X_tr = X_tr.drop(['label'], axis=1)\r\n    \r\n    y_val = X_val['label'].values\r\n    X_val = X_val.drop(['label'], axis=1)\r\n    \r\n    print('\\ny_tr distribution: {}'.format(Counter(y_tr)))\r\n    \r\n    d_train = lgb.Dataset(X_tr, label=y_tr) \r\n    d_valid = lgb.Dataset(X_val, label=y_val)\r\n    watchlist = [d_train, d_valid]\r\n    \r\n    print('training LGB:')\r\n    model = lgb.train(params,\r\n                      train_set=d_train,\r\n                      num_boost_round=50,\r\n                      valid_sets=watchlist,\r\n                      verbose_eval=1,\r\n                      early_stopping_rounds=5)\r\n    \r\n    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\r\n    test_pred = model.predict(test.drop(['query','title','query_id','query_title_id'],axis=1), num_iteration=model.best_iteration)\r\n    \r\n    oof_train[valid_index] = val_pred\r\n    oof_test[:, i] = test_pred\r\n    i += 1\r\nmodel.save_model('lgb.model')  # 用于存储训练出的模型\r\n\r\nres = oof_test.mean(axis=1) ","execution_count":11},{"metadata":{"id":"F259FC13AF484BE592DB45F092A1CB59","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"Start training...\n[1]\ttraining's binary_logloss: 0.562283\tvalid_1's binary_logloss: 0.562158\nTraining until validation scores don't improve for 5 rounds.\n[2]\ttraining's binary_logloss: 0.55836\tvalid_1's binary_logloss: 0.558237\n[3]\ttraining's binary_logloss: 0.555599\tvalid_1's binary_logloss: 0.555484\n[4]\ttraining's binary_logloss: 0.553701\tvalid_1's binary_logloss: 0.553584\n[5]\ttraining's binary_logloss: 0.552322\tvalid_1's binary_logloss: 0.552205\n[6]\ttraining's binary_logloss: 0.551287\tvalid_1's binary_logloss: 0.551185\n[7]\ttraining's binary_logloss: 0.550455\tvalid_1's binary_logloss: 0.550357\n[8]\ttraining's binary_logloss: 0.549845\tvalid_1's binary_logloss: 0.549752\n[9]\ttraining's binary_logloss: 0.549364\tvalid_1's binary_logloss: 0.549279\n[10]\ttraining's binary_logloss: 0.548962\tvalid_1's binary_logloss: 0.54888\n[11]\ttraining's binary_logloss: 0.548605\tvalid_1's binary_logloss: 0.548527\n[12]\ttraining's binary_logloss: 0.548008\tvalid_1's binary_logloss: 0.547927\n[13]\ttraining's binary_logloss: 0.547614\tvalid_1's binary_logloss: 0.547537\n[14]\ttraining's binary_logloss: 0.547389\tvalid_1's binary_logloss: 0.547318\n[15]\ttraining's binary_logloss: 0.547187\tvalid_1's binary_logloss: 0.547125\n[16]\ttraining's binary_logloss: 0.547044\tvalid_1's binary_logloss: 0.54699\n[17]\ttraining's binary_logloss: 0.546907\tvalid_1's binary_logloss: 0.546857\n[18]\ttraining's binary_logloss: 0.546777\tvalid_1's binary_logloss: 0.546736\n[19]\ttraining's binary_logloss: 0.546489\tvalid_1's binary_logloss: 0.546447\n[20]\ttraining's binary_logloss: 0.546354\tvalid_1's binary_logloss: 0.546315\n[21]\ttraining's binary_logloss: 0.546081\tvalid_1's binary_logloss: 0.546048\n[22]\ttraining's binary_logloss: 0.545937\tvalid_1's binary_logloss: 0.545909\n[23]\ttraining's binary_logloss: 0.545484\tvalid_1's binary_logloss: 0.54545\n[24]\ttraining's binary_logloss: 0.545407\tvalid_1's binary_logloss: 0.545383\n[25]\ttraining's binary_logloss: 0.545322\tvalid_1's binary_logloss: 0.545303\n[26]\ttraining's binary_logloss: 0.545204\tvalid_1's binary_logloss: 0.545195\n[27]\ttraining's binary_logloss: 0.545143\tvalid_1's binary_logloss: 0.545125\n[28]\ttraining's binary_logloss: 0.544871\tvalid_1's binary_logloss: 0.544849\n[29]\ttraining's binary_logloss: 0.544809\tvalid_1's binary_logloss: 0.544796\n[30]\ttraining's binary_logloss: 0.544607\tvalid_1's binary_logloss: 0.544595\n[31]\ttraining's binary_logloss: 0.544536\tvalid_1's binary_logloss: 0.544528\n[32]\ttraining's binary_logloss: 0.544303\tvalid_1's binary_logloss: 0.544294\n[33]\ttraining's binary_logloss: 0.544243\tvalid_1's binary_logloss: 0.544239\n[34]\ttraining's binary_logloss: 0.544114\tvalid_1's binary_logloss: 0.544114\n[35]\ttraining's binary_logloss: 0.544077\tvalid_1's binary_logloss: 0.544054\n[36]\ttraining's binary_logloss: 0.544033\tvalid_1's binary_logloss: 0.544043\n[37]\ttraining's binary_logloss: 0.543879\tvalid_1's binary_logloss: 0.543898\n[38]\ttraining's binary_logloss: 0.543817\tvalid_1's binary_logloss: 0.543829\n[39]\ttraining's binary_logloss: 0.543693\tvalid_1's binary_logloss: 0.543709\n[40]\ttraining's binary_logloss: 0.543654\tvalid_1's binary_logloss: 0.543674\n[41]\ttraining's binary_logloss: 0.543537\tvalid_1's binary_logloss: 0.543561\n[42]\ttraining's binary_logloss: 0.543408\tvalid_1's binary_logloss: 0.543432\n[43]\ttraining's binary_logloss: 0.543349\tvalid_1's binary_logloss: 0.543378\n[44]\ttraining's binary_logloss: 0.543314\tvalid_1's binary_logloss: 0.543344\n[45]\ttraining's binary_logloss: 0.54326\tvalid_1's binary_logloss: 0.543299\n[46]\ttraining's binary_logloss: 0.543229\tvalid_1's binary_logloss: 0.543264\n[47]\ttraining's binary_logloss: 0.54318\tvalid_1's binary_logloss: 0.543215\n[48]\ttraining's binary_logloss: 0.543149\tvalid_1's binary_logloss: 0.543189\n[49]\ttraining's binary_logloss: 0.54311\tvalid_1's binary_logloss: 0.543153\n[50]\ttraining's binary_logloss: 0.543028\tvalid_1's binary_logloss: 0.543071\nDid not meet early stopping. Best iteration is:\n[50]\ttraining's binary_logloss: 0.543028\tvalid_1's binary_logloss: 0.543071\n","name":"stdout"}],"source":"\n# 创建成lgb特征的数据集格式\nlgb_train = lgb.Dataset(X_train.drop(['query_id','query_title_id','query','title'],axis=1), y_train)\n#If this is Dataset for validation, training data should be used as reference.\nlgb_eval = lgb.Dataset(X_test.drop(['query_id','query_title_id','query','title'],axis=1), y_test, reference=lgb_train)\n\n#lgb_train = lgb.Dataset(X_train.drop(['query','title'],axis=1), label=y_train)\n#lgb_eval = lgb.Dataset(X_test.drop(['query','title'],axis=1), label=y_test, reference=lgb_train)\n\n# lgb_train = lgb.Dataset(X_train, y_train)\n# lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n# 将参数写成字典下形式\nparams = {\n    'task': 'train',\n    'boosting_type': 'gbdt',  # 设置提升类型\n    'objective': 'regression', # 目标函数\n    'metric': {'binary_logloss'},  # 评估函数\n    # 'num_leaves': 60,   # 叶子节点数\n    # 'max_depth':5,   #最大数的深度\n    'learning_rate': 0.2,  # 学习速率0.05 0.1\n    # 'feature_fraction': 0.9, # 建树的特征选择比例\n    # 'bagging_fraction': 0.9, # 建树的样本采样比例\n    'bagging_freq': 5,  # k 意味着每 k 次迭代执行bagging\n    #  'lambda': 10,\n    'verbose': 1 # <0 显示致命的, =0 显示错误 (警告), >0 显示信息\n}\nwatchlist = [lgb_train, lgb_eval]\n# watchlist = [(lgb_train, 'train'), (lgb_eval, 'valid')]\nprint('Start training...')\n# 训练 cv and train\ngbm = lgb.train(params,lgb_train,num_boost_round=50,valid_sets = watchlist,early_stopping_rounds=5)\n\n# print('Save model...')\n# 保存模型到文件\n#gbm.save_model('LightGBM.txt')\n\n#print('Start predicting...')\n\n# 预测数据集\n#y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n#auc 0.544 \n","execution_count":7},{"metadata":{"id":"63EFA1EDDEDE4AE9802D34E69CF10760","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"q_auc invalid_count = 363\n","name":"stdout"},{"output_type":"execute_result","metadata":{},"data":{"text/plain":"0.5351825462797781"},"execution_count":10}],"source":"def q_auc(df):\n    invalid_count = 0\n    query_ids = df['query_id'].unique()\n    aucs = []\n    for query_id in query_ids:\n        current = df[df['query_id'] == query_id]\n        if sum(current['label']) not in [0, len(current['label'])]:\n            auc = roc_auc_score(current['label'], current['prediction'])  # 这里计算qauc\n        else:\n            auc = 0.5\n            invalid_count += 1\n        aucs.append(auc)\n    print(f'q_auc invalid_count = {invalid_count}')\n    return np.mean(aucs)\nx = train[:100000]\n# x = train[:10000]\n# x.drop(['query_id','query_title_id','title','query','label'],axis=1)\npred = gbm.predict(x.drop(['query_id','query_title_id','title','query','label'],axis=1), num_iteration=gbm.best_iteration)\nx['prediction'] = pred\n#qauc 0.537-> 0.536-> 0.538-> 0.544-> 0.545-> 0.547->0.564(特征工程)->0.561（lambda 10 正则）\nq_auc(x) ","execution_count":10},{"metadata":{"id":"ACF206A00721462E80FDF6BC00FE4F80","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"import seaborn as sns\n# feature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_,X.columns)), columns=['Value','Feature'])\nplt.figure(figsize=(20, 10))\nimportance = pd.DataFrame({'column': gbm.feature_name(),'importance': gbm.feature_importance(),}).sort_values(by='importance',ascending=False)\nsns.barplot(x=\"importance\", y=\"column\", data=importance)\nplt.title('LightGBM Features Importance')\nplt.tight_layout()\nplt.show()\n\n# importance.plot()","execution_count":8},{"metadata":{"id":"B8CB026CA2D3477FAC17B07F8D87CB9B","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"importance\n","execution_count":24},{"metadata":{"id":"A578FD1C2AE1470B9A4468101ACF37C0","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# test['prediction'] = gbm.predict(test.drop(['query_id','query_title_id','title','query'],axis=1))\ntest['prediction'] = res\nsub = test[['query_id','query_title_id','prediction']]\nsub.to_csv('lgb_1.csv',index=False,header=False)\n","execution_count":null},{"metadata":{"id":"5B23668B84214D4FAB936B566ADC93DD","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"!head -n 10 lgb_1.csv","execution_count":23},{"metadata":{"id":"901D02BE92044224991CC9805AC88680","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 下载提交工具至当前目录，仅需执行一次（ XXXXXXXXX 为团队 Token）\r\n!wget -nv -O kesci_submit https://www.heywhale.com/kesci_submit&&chmod +x kesci_submit\r\n# 提交文件 my_submission.csv 进行评审；\r\n!./kesci_submit -token 2ebe9c16d35d9efc -file lgb_1.csv","execution_count":null},{"metadata":{"id":"D89AD6517EB04BEA85B4BB03D0B68652"},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}