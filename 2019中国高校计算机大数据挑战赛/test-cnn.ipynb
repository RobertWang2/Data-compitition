{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义加载数据的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "def process_data(path):\n",
    "\n",
    "    # 现在是使用10M个进行尝试\n",
    "    tra_data = pd.read_csv(\n",
    "        path,\n",
    "        header=None,\n",
    "        names=['query_id', 'query', 'query_title_id', 'title', 'label'],\n",
    "        iterator=True\n",
    "    )\n",
    "    chunks = tra_data.get_chunk(2500000)\n",
    "    tra_num = 2000000\n",
    "    test_num = 500000\n",
    "    train_text = [s.split() for s in chunks['title'][:tra_num]]\n",
    "    test_text = [s.split() for s in chunks['title']\n",
    "                 [tra_num: tra_num + test_num]]\n",
    "\n",
    "    train_y = chunks['label'][:tra_num]\n",
    "    test_y = chunks['label'][tra_num: tra_num + test_num]\n",
    "    del(chunks)\n",
    "    \n",
    "    submit_data = pd.read_csv(\n",
    "                    \"/home/kesci/input/bytedance/first-round/test.csv\",\n",
    "                    header=None,\n",
    "                    names=['query_id', 'query', 'query_title_id', 'title']\n",
    "                    )\n",
    "    submit_text = [s.split() for s in submit_data['title']]\n",
    "    \n",
    "    \n",
    "    return train_text, test_text, train_y, test_y, submit_text, submit_data[['query_id', 'query_title_id']]\n",
    "\n",
    "\n",
    "def pad_sentences(sentences, sequence_length=20, padding_word=\"<PAD/>\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    # max_length = max(len(x) for x in sentences)\n",
    "    # ave_length = sum(len(x) for x in sentences)/len(sentences)\n",
    "    # print('max length', max_length)\n",
    "    # print('ave length', ave_length)\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        if num_padding >= 0:\n",
    "            new_sentence = sentence + [padding_word] * num_padding\n",
    "            padded_sentences.append(new_sentence)\n",
    "        else:\n",
    "            padded_sentences.append(sentence[:sequence_length])\n",
    "    return padded_sentences, sequence_length\n",
    "\n",
    "\n",
    "def build_vocab(sentences, min_count=100):\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0]\n",
    "                      for x in word_counts.most_common() if x[1] >= min_count]\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return [vocabulary, vocabulary_inv]\n",
    "\n",
    "\n",
    "def build_input_data_generator(sentences, labels, vocabulary, batch_size=50000, samples_per_epoch=2000000):\n",
    "    steps_per_epoch=int(samples_per_epoch/batch_size)\n",
    "    while 1:\n",
    "        inx = 0\n",
    "        for i in range(steps_per_epoch):\n",
    "            batch_x = sentences[inx:inx+batch_size]\n",
    "            batch_y = labels[inx:inx+batch_size]\n",
    "            inx += batch_size\n",
    "            # create Numpy arrays of input data\n",
    "            # and labels, from each line in the file\n",
    "            x = np.array([[vocabulary.get(word, 0) for word in sentence]\n",
    "                  for sentence in batch_x])\n",
    "            y = np.array(batch_y)\n",
    "            # x, y = process_line(line)\n",
    "            yield (x, y)\n",
    "            \n",
    "\n",
    "def build_test_4_input(vocabulary, len1, len2, len3, len4):\n",
    "    test1 = pd.read_csv(\n",
    "                    \"/home/kesci/input/bytedance/first-round/test.csv\",\n",
    "                    header=None, \n",
    "                    names=['query_id', 'query', 'query_title_id', 'title'], \n",
    "                    ) \n",
    "    \n",
    "    print('loading 4 input test data .....')\n",
    "    len1 = len1\n",
    "    len2 = len2\n",
    "    len3 = len3\n",
    "    len4 = len4\n",
    "    sens1 = [s.split() for s in test1['query']]\n",
    "    pad1, _ = pad_sentences(sens1, len1)\n",
    "    sens2 = [s.split() for s in test1['title']]\n",
    "    pad2, _ = pad_sentences(sens2, len2)\n",
    "    del(sens1, sens2)\n",
    "    sub_info = test1[['query_id', 'query_title_id']]\n",
    "    del(test1)\n",
    "    test2 = pd.read_csv(\n",
    "                    \"/home/kesci/work/test_and_diff_text.csv\",\n",
    "                    ) \n",
    "    sens3 = [str(s).split() for s in test2['andText']]\n",
    "    print('padding 2-------')\n",
    "    pad3, _ = pad_sentences(sens3, len3)\n",
    "    del(sens3)\n",
    "    sens4 = [str(s).split() for s in test2['diffText']]\n",
    "    del(test2)\n",
    "    pad4, _ = pad_sentences(sens4, len4)\n",
    "    del(sens4)\n",
    "    print('transfer ------')\n",
    "    x1 = np.array([[vocabulary.get(word, 0) for word in sentence] for sentence in pad1])\n",
    "    del(pad1)\n",
    "    x2 = np.array([[vocabulary.get(word, 0) for word in sentence] for sentence in pad2])\n",
    "    del(pad2)\n",
    "    x3 = np.array([[vocabulary.get(word, 0) for word in sentence] for sentence in pad3])\n",
    "    del(pad3)\n",
    "    x4 = np.array([[vocabulary.get(word, 0) for word in sentence] for sentence in pad4])\n",
    "    del(pad4)\n",
    "    x = [x1, x2, x3, x4]\n",
    "    return x, sub_info\n",
    "\n",
    "\n",
    "def generator_4_input(vocabulary, len1, len2, len3, len4, batch_size=50000, samples=1000000):\n",
    "    steps_per_epoch=int(samples/batch_size)\n",
    "    # chunks_1 = pd.read_csv(\n",
    "    #                 \"/home/kesci/input/bytedance/first-round/train.csv\",\n",
    "    #                 header=None, \n",
    "    #                 names=['query_id', 'query', 'query_title_id', 'title', 'label'], \n",
    "    #                 iterator=True\n",
    "    #                 ) \n",
    "    # chunks_2 = pd.read_csv(\n",
    "    #                 \"/home/kesci/work/15M_and_diff_text.csv\",\n",
    "    #                 iterator=True\n",
    "    #                 ) \n",
    "    while 1:\n",
    "        chunks_1 = pd.read_csv(\n",
    "                    \"/home/kesci/input/bytedance/first-round/train.csv\",\n",
    "                    header=None, \n",
    "                    names=['query_id', 'query', 'query_title_id', 'title', 'label'], \n",
    "                    iterator=True\n",
    "                    ) \n",
    "        chunks_2 = pd.read_csv(\n",
    "                    \"/home/kesci/work/15M_and_diff_text.csv\",\n",
    "                    iterator=True\n",
    "                    ) \n",
    "        # for j in range(7):\n",
    "        #     chunks_1.get_chunk(1000000)\n",
    "        #     chunks_2.get_chunk(1000000)\n",
    "        # print('从700万开始')\n",
    "        len1 = len1\n",
    "        len2 = len2\n",
    "        len3 = len3\n",
    "        len4 = len4\n",
    "        for i in range(steps_per_epoch):\n",
    "            x = chunks_1.get_chunk(batch_size)\n",
    "            sen1 = x['query']\n",
    "            sen2 = x['title']\n",
    "            label = x['label']\n",
    "            chunk2 = chunks_2.get_chunk(batch_size)\n",
    "            # print(sen1.)\n",
    "            sens1 = [s.split() for s in sen1]\n",
    "            sens2 = [s.split() for s in sen2]\n",
    "            sens3 = [str(s).split() for s in chunk2['andText']]\n",
    "            sens4 = [str(s).split() for s in chunk2['diffText']]\n",
    "\n",
    "            pad1, _ = pad_sentences(sens1, len1)\n",
    "            pad2, _ = pad_sentences(sens2, len2)\n",
    "            pad3, _ = pad_sentences(sens3, len3)\n",
    "            pad4, _ = pad_sentences(sens4, len4)\n",
    "\n",
    "            # create Numpy arrays of input data\n",
    "            # and labels, from each line in the file\n",
    "            x1 = np.array([[vocabulary.get(word, 0) for word in sentence]\n",
    "                  for sentence in pad1])\n",
    "            x2 = np.array([[vocabulary.get(word, 0) for word in sentence]\n",
    "                  for sentence in pad2])\n",
    "            x3 = np.array([[vocabulary.get(word, 0) for word in sentence]\n",
    "                  for sentence in pad3])\n",
    "            x4 = np.array([[vocabulary.get(word, 0) for word in sentence]\n",
    "                  for sentence in pad4])\n",
    "            y = np.array(label)\n",
    "            x = [x1, x2, x3, x4]\n",
    "            # x, y = process_line(line)\n",
    "            yield (x, y)\n",
    "         \n",
    "        \n",
    "def build_valid_4_input(vocabulary, len1, len2, len3, len4):\n",
    "\n",
    "    chunks_1 = pd.read_csv(\n",
    "                    \"/home/kesci/input/bytedance/first-round/train.csv\",\n",
    "                    header=None, \n",
    "                    names=['query_id', 'query', 'query_title_id', 'title', 'label'], \n",
    "                    iterator=True\n",
    "                    ) \n",
    "    chunks_2 = pd.read_csv(\n",
    "                    \"/home/kesci/work/15M_and_diff_text.csv\",\n",
    "                    iterator=True\n",
    "                    ) \n",
    "    for j in range(14):\n",
    "        chunks_1.get_chunk(1000000)\n",
    "        chunks_2.get_chunk(1000000)\n",
    "    \n",
    "    print('从1400万开始取valid data ......')\n",
    "    len1 = len1\n",
    "    len2 = len2\n",
    "    len3 = len3\n",
    "    len4 = len4\n",
    "    \n",
    "    # 1400W 到 1500W作为valid\n",
    "    x = chunks_1.get_chunk(1000000)\n",
    "    sen1 = x['query']\n",
    "    sen2 = x['title']\n",
    "    label = x['label']\n",
    "    chunk2 = chunks_2.get_chunk(1000000)\n",
    "            # print(sen1.)\n",
    "    sens1 = [s.split() for s in sen1]\n",
    "    sens2 = [s.split() for s in sen2]\n",
    "    sens3 = [str(s).split() for s in chunk2['andText']]\n",
    "    sens4 = [str(s).split() for s in chunk2['diffText']]\n",
    "\n",
    "    pad1, _ = pad_sentences(sens1, len1)\n",
    "    pad2, _ = pad_sentences(sens2, len2)\n",
    "    pad3, _ = pad_sentences(sens3, len3)\n",
    "    pad4, _ = pad_sentences(sens4, len4)\n",
    "\n",
    "            # create Numpy arrays of input data\n",
    "            # and labels, from each line in the file\n",
    "    x1 = np.array([[vocabulary.get(word, 0) for word in sentence]\n",
    "                  for sentence in pad1])\n",
    "    x2 = np.array([[vocabulary.get(word, 0) for word in sentence]\n",
    "                  for sentence in pad2])\n",
    "    x3 = np.array([[vocabulary.get(word, 0) for word in sentence]\n",
    "                  for sentence in pad3])\n",
    "    x4 = np.array([[vocabulary.get(word, 0) for word in sentence]\n",
    "                  for sentence in pad4])\n",
    "    y = np.array(label)\n",
    "    x = [x1, x2, x3, x4]\n",
    "    print('build valid data over ......')\n",
    "        # x, y = process_line(line)\n",
    "    return (x, y)\n",
    "\n",
    "\n",
    "def build_test_2_input(vocabulary):\n",
    "    test1 = pd.read_csv(\n",
    "                    \"/home/kesci/input/bytedance/first-round/test.csv\",\n",
    "                    header=None, \n",
    "                    names=['query_id', 'query', 'query_title_id', 'title'], \n",
    "                    ) \n",
    "    \n",
    "    print('loding')\n",
    "    len1 = 20\n",
    "    len2 = 10\n",
    "    len3 = 15\n",
    "    # sens1 = [s.split() for s in test1['query']]\n",
    "    # pad1, _ = pad_sentences(sens1, len1)\n",
    "    # del(sens1)\n",
    "    sub_info = test1[['query_id', 'query_title_id']]\n",
    "    del(test1)\n",
    "    test2 = pd.read_csv(\n",
    "                    \"/home/kesci/work/test_and_diff_text.csv\",\n",
    "                    ) \n",
    "    sens2 = [str(s).split() for s in test2['andText']]\n",
    "    print('padding 2-------')\n",
    "    pad2, _ = pad_sentences(sens2, len2)\n",
    "    del(sens2)\n",
    "    sens3 = [str(s).split() for s in test2['diffText']]\n",
    "    del(test2)\n",
    "    pad3, _ = pad_sentences(sens3, len3)\n",
    "    del(sens3)\n",
    "    print('transfer ------')\n",
    "    # x1 = np.array([[vocabulary.get(word, 0) for word in sentence] for sentence in pad1])\n",
    "    # del(pad1)\n",
    "    x2 = np.array([[vocabulary.get(word, 0) for word in sentence] for sentence in pad2])\n",
    "    del(pad2)\n",
    "    x3 = np.array([[vocabulary.get(word, 0) for word in sentence] for sentence in pad3])\n",
    "    del(pad3)\n",
    "    x = [x2, x3]\n",
    "    return x, sub_info\n",
    "    \n",
    "    \n",
    "def build_test_3_input(vocabulary, len1, len2, len3):\n",
    "    test1 = pd.read_csv(\n",
    "                    \"/home/kesci/input/bytedance/first-round/test.csv\",\n",
    "                    header=None, \n",
    "                    names=['query_id', 'query', 'query_title_id', 'title'], \n",
    "                    ) \n",
    "    \n",
    "    print('loding')\n",
    "    len1 = len1\n",
    "    len2 = len2\n",
    "    len3 = len3\n",
    "    sens1 = [s.split() for s in test1['query']]\n",
    "    pad1, _ = pad_sentences(sens1, len1)\n",
    "    del(sens1)\n",
    "    sub_info = test1[['query_id', 'query_title_id']]\n",
    "    del(test1)\n",
    "    test2 = pd.read_csv(\n",
    "                    \"/home/kesci/work/test_and_diff_text.csv\",\n",
    "                    ) \n",
    "    sens2 = [str(s).split() for s in test2['andText']]\n",
    "    print('padding 2-------')\n",
    "    pad2, _ = pad_sentences(sens2, len2)\n",
    "    del(sens2)\n",
    "    sens3 = [str(s).split() for s in test2['diffText']]\n",
    "    del(test2)\n",
    "    pad3, _ = pad_sentences(sens3, len3)\n",
    "    del(sens3)\n",
    "    print('transfer ------')\n",
    "    x1 = np.array([[vocabulary.get(word, 0) for word in sentence] for sentence in pad1])\n",
    "    del(pad1)\n",
    "    x2 = np.array([[vocabulary.get(word, 0) for word in sentence] for sentence in pad2])\n",
    "    del(pad2)\n",
    "    x3 = np.array([[vocabulary.get(word, 0) for word in sentence] for sentence in pad3])\n",
    "    del(pad3)\n",
    "    x = [x1, x2, x3]\n",
    "    return x, sub_info\n",
    "\n",
    "\n",
    "\n",
    "def generator_3_input(vocabulary, len1, len2, len3, batch_size=50000, samples=1000000):\n",
    "    steps_per_epoch=int(samples/batch_size)\n",
    "    # chunks_1 = pd.read_csv(\n",
    "    #                 \"/home/kesci/input/bytedance/first-round/train.csv\",\n",
    "    #                 header=None, \n",
    "    #                 names=['query_id', 'query', 'query_title_id', 'title', 'label'], \n",
    "    #                 iterator=True\n",
    "    #                 ) \n",
    "    # chunks_2 = pd.read_csv(\n",
    "    #                 \"/home/kesci/work/15M_and_diff_text.csv\",\n",
    "    #                 iterator=True\n",
    "    #                 ) \n",
    "    while 1:\n",
    "        chunks_1 = pd.read_csv(\n",
    "                    \"/home/kesci/input/bytedance/first-round/train.csv\",\n",
    "                    header=None, \n",
    "                    names=['query_id', 'query', 'query_title_id', 'title', 'label'], \n",
    "                    iterator=True\n",
    "                    ) \n",
    "        chunks_2 = pd.read_csv(\n",
    "                    \"/home/kesci/work/15M_and_diff_text.csv\",\n",
    "                    iterator=True\n",
    "                    ) \n",
    "        # for j in range(7):\n",
    "        #     chunks_1.get_chunk(1000000)\n",
    "        #     chunks_2.get_chunk(1000000)\n",
    "        # print('从700万开始')\n",
    "        len1 = len1\n",
    "        len2 = len2\n",
    "        len3 = len3\n",
    "        for i in range(steps_per_epoch):\n",
    "            x = chunks_1.get_chunk(batch_size)\n",
    "            sen1 = x['query']\n",
    "            label = x['label']\n",
    "            sen2 = chunks_2.get_chunk(batch_size)\n",
    "            # print(sen1.)\n",
    "            sens1 = [s.split() for s in sen1]\n",
    "            sens2 = [str(s).split() for s in sen2['andText']]\n",
    "            sens3 = [str(s).split() for s in sen2['diffText']]\n",
    "\n",
    "            pad1, _ = pad_sentences(sens1, len1)\n",
    "            pad2, _ = pad_sentences(sens2, len2)\n",
    "            pad3, _ = pad_sentences(sens3, len3)\n",
    "\n",
    "            # create Numpy arrays of input data\n",
    "            # and labels, from each line in the file\n",
    "            x1 = np.array([[vocabulary.get(word, 0) for word in sentence]\n",
    "                  for sentence in pad1])\n",
    "            x2 = np.array([[vocabulary.get(word, 0) for word in sentence]\n",
    "                  for sentence in pad2])\n",
    "            x3 = np.array([[vocabulary.get(word, 0) for word in sentence]\n",
    "                  for sentence in pad3])\n",
    "            y = np.array(label)\n",
    "            x = [x1, x2, x3]\n",
    "            # x, y = process_line(line)\n",
    "            yield (x, y)\n",
    "\n",
    "            \n",
    "def generator_2_input(vocabulary, batch_size=50000, samples=1000000):\n",
    "    steps_per_epoch=int(samples/batch_size)\n",
    "\n",
    "    while 1:\n",
    "        chunks_1 = pd.read_csv(\n",
    "                    \"/home/kesci/input/bytedance/first-round/train.csv\",\n",
    "                    header=None, \n",
    "                    names=['query_id', 'query', 'query_title_id', 'title', 'label'], \n",
    "                    iterator=True\n",
    "                    ) \n",
    "        chunks_2 = pd.read_csv(\n",
    "                    \"/home/kesci/work/15M_and_diff_text.csv\",\n",
    "                    iterator=True\n",
    "                    ) \n",
    "        # for j in range(7):\n",
    "        #     chunks_1.get_chunk(1000000)\n",
    "        #     chunks_2.get_chunk(1000000)\n",
    "        # print('从700万开始')\n",
    "        for i in range(steps_per_epoch):\n",
    "            label = chunks_1.get_chunk(batch_size)['label']\n",
    "            # sen1 = x['query']\n",
    "            # label = x['label']\n",
    "            sen2 = chunks_2.get_chunk(batch_size)\n",
    "            # print(sen1.)\n",
    "            # sens1 = [s.split() for s in sen1]\n",
    "            sens2 = [str(s).split() for s in sen2['andText']]\n",
    "            sens3 = [str(s).split() for s in sen2['diffText']]\n",
    "            # len1 = 20\n",
    "            len2 = 10\n",
    "            len3 = 15\n",
    "            # pad1, _ = pad_sentences(sens1, len1)\n",
    "            pad2, _ = pad_sentences(sens2, len2)\n",
    "            pad3, _ = pad_sentences(sens3, len3)\n",
    "\n",
    "            # create Numpy arrays of input data\n",
    "            # and labels, from each line in the file\n",
    "            # x1 = np.array([[vocabulary.get(word, 0) for word in sentence]\n",
    "                #   for sentence in pad1])\n",
    "            x2 = np.array([[vocabulary.get(word, 0) for word in sentence]\n",
    "                  for sentence in pad2])\n",
    "            x3 = np.array([[vocabulary.get(word, 0) for word in sentence]\n",
    "                  for sentence in pad3])\n",
    "            y = np.array(label)\n",
    "            x = [x2, x3]\n",
    "            # x, y = process_line(line)\n",
    "            yield (x, y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义NN层，Keras based on tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named tensorflow",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5fcde668118d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGlobalAveragePooling1D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGlobalMaxPooling1D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSpatialDropout1D\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGRU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPooling1D\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named tensorflow"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, SpatialDropout1D\n",
    "from keras.engine import Layer\n",
    "from keras.layers import Activation, Add, Bidirectional, Conv1D, Dense, Dropout, Embedding, Flatten\n",
    "from keras.layers import concatenate, GRU, Input, K, LSTM, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import MaxPool1D\n",
    "from keras.engine import InputSpec\n",
    "from keras.layers.pooling import MaxPool1D, AveragePooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "\n",
    "def TextCNN(sequence_length,\n",
    "            num_filters,\n",
    "            voc,\n",
    "            dense_size,\n",
    "            drop_out_rate=0.4,\n",
    "            embedding_size=200,\n",
    "            filter_sizes=[3, 4, 5]\n",
    "            ):\n",
    "        # input layer\n",
    "        input1 = Input(shape=(sequence_length,))\n",
    "        embed_layer = Embedding(len(voc), embedding_size, trainable=True)(input1)\n",
    "        embed_layer = SpatialDropout1D(0.3)(embed_layer)\n",
    "        # create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for filter_size in filter_sizes:\n",
    "            x = Conv1D(num_filters, filter_size, activation='relu')(embed_layer)\n",
    "            x = MaxPool1D(int(x.shape[1]))(x)\n",
    "            pooled_outputs.append(x)\n",
    "        merged = concatenate(pooled_outputs)\n",
    "        x = Flatten()(merged)\n",
    "        drop = Dropout(drop_out_rate)(x)\n",
    "        outputs = Dense(dense_size, activation='sigmoid')(drop)\n",
    "\n",
    "        model = Model(input1, outputs)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[auc_score, 'accuracy'])\n",
    "        return model\n",
    "\n",
    "\n",
    "class KMaxPooling(Layer):\n",
    "    \"\"\"\n",
    "    k-max-pooling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_spec = InputSpec(ndim=3)\n",
    "        self.k = k\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], (input_shape[2] * self.k))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        shifted_input = tf.transpose(inputs, [0, 2, 1])\n",
    "        top_k = tf.nn.top_k(shifted_input, k=self.k, sorted=True, name=None)[0]\n",
    "        # return flattened output\n",
    "        # print(top_k.shape)\n",
    "        return Flatten()(top_k)\n",
    "\n",
    "    \n",
    "def simple_2_input(len_2, len_3, voc, dropout_p, num_class=1, drop_out_p=0.5, embedding_size=128): \n",
    "    '''\n",
    "    使用差集与交集两个输入\n",
    "    '''\n",
    "    input2 = Input(shape=(len_2,))\n",
    "    input3 = Input(shape=(len_3,))\n",
    "    embedder = Embedding(len(voc), embedding_size, trainable=True)\n",
    "    embedder2 = Embedding(len(voc), embedding_size, trainable=True)\n",
    "    embed2 = embedder(input2)\n",
    "    embed3 = embedder2(input3)\n",
    "\n",
    "    ave2 = GlobalAveragePooling1D()(embed2)\n",
    "    ave3 = GlobalAveragePooling1D()(embed3)\n",
    "\n",
    "    max2 = GlobalMaxPooling1D()(embed2)\n",
    "    max3 = GlobalMaxPooling1D()(embed3)\n",
    "    con = concatenate([ave2, max2, ave3, max3])\n",
    "    output = Dense(100)(con)\n",
    "    output = Dropout(drop_out_p)(output)\n",
    "    main_output = Dense(num_class, activation='sigmoid')(output)\n",
    "    model = Model(inputs=[input2, input3], outputs=main_output)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[auc_score, 'accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def simple_4_input(len_1, len_2, len_3, len_4, voc, dropout_p, num_class=1, drop_out_p=0.5, embedding_size=128):\n",
    "    '''\n",
    "    使用query title 交集 差集4个输入\n",
    "    '''\n",
    "    input1 = Input(shape=(len_1,))\n",
    "    input2 = Input(shape=(len_2,))\n",
    "    input3 = Input(shape=(len_3,))\n",
    "    input4 = Input(shape=(len_4,))\n",
    "    embedder = Embedding(len(voc), embedding_size, trainable=True)\n",
    "    embed1 = embedder(input1)\n",
    "    embed2 = embedder(input2)\n",
    "    embed3 = embedder(input3)\n",
    "    embed4 = embedder(input4)\n",
    "    ave1 = GlobalAveragePooling1D()(embed1)\n",
    "    ave2 = GlobalAveragePooling1D()(embed2)\n",
    "    ave3 = GlobalAveragePooling1D()(embed3)\n",
    "    ave4 = GlobalAveragePooling1D()(embed4)\n",
    "    max1 = GlobalMaxPooling1D()(embed1)\n",
    "    max2 = GlobalMaxPooling1D()(embed2)\n",
    "    max3 = GlobalMaxPooling1D()(embed3)\n",
    "    max4 = GlobalMaxPooling1D()(embed4)\n",
    "    con = concatenate([ave1, ave2, ave3, ave4, max1, max2, max3, max4])\n",
    "    output = Dense(100)(con)\n",
    "    output = Dropout(drop_out_p)(output)\n",
    "    main_output = Dense(num_class, activation='sigmoid')(output)\n",
    "    model = Model(inputs=[input1, input2, input3, input4], outputs=main_output)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[auc_score, 'accuracy'])\n",
    "    return model\n",
    "\n",
    "def simple_3_input(len_1, len_2, len_3, voc, dropout_p, num_class=1, drop_out_p=0.5, embedding_size=128):\n",
    "    '''\n",
    "    使用query 交集 差集3个输入\n",
    "    '''\n",
    "    input1 = Input(shape=(len_1,))\n",
    "    input2 = Input(shape=(len_2,))\n",
    "    input3 = Input(shape=(len_3,))\n",
    "    embedder = Embedding(len(voc), embedding_size, trainable=True)\n",
    "    embed1 = embedder(input1)\n",
    "    embed2 = embedder(input2)\n",
    "    embed3 = embedder(input3)\n",
    "    ave1 = GlobalAveragePooling1D()(embed1)\n",
    "    ave2 = GlobalAveragePooling1D()(embed2)\n",
    "    ave3 = GlobalAveragePooling1D()(embed3)\n",
    "    max1 = GlobalMaxPooling1D()(embed1)\n",
    "    max2 = GlobalMaxPooling1D()(embed2)\n",
    "    max3 = GlobalMaxPooling1D()(embed3)\n",
    "    con = concatenate([ave1, max1, ave2, max2, ave3, max3])\n",
    "    output = Dense(100)(con)\n",
    "    output = Dropout(drop_out_p)(output)\n",
    "    main_output = Dense(num_class, activation='sigmoid')(output)\n",
    "    model = Model(inputs=[input1, input2, input3], outputs=main_output)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[auc_score, 'accuracy'])\n",
    "    return model\n",
    "    \n",
    "    \n",
    "def simpleLayer(sequence_length, voc, dropout_p, num_class=1, drop_out_p=0.5, embedding_size=128, chose='1'):\n",
    "    main_input = Input(shape=(sequence_length,), dtype='float64')\n",
    "    # 词嵌入（使用预训练的词向量）\n",
    "    embedder = Embedding(len(voc), embedding_size, input_length=sequence_length,\n",
    "                         trainable=True)\n",
    "    # embedder2 = Embedding(len(voc), embedding_size, input_length=sequence_length,\n",
    "    #                       trainable=True)\n",
    "    embed = embedder(main_input)\n",
    "    # embed2 = embedder2(main_input)\n",
    "    var = VariancePooling()(embed)\n",
    "    new_ave = GlobalAveragePooling1D()(embed)\n",
    "    new_max = GlobalMaxPooling1D()(embed)\n",
    "    topK_max = KMaxPooling(k=2)(embed)\n",
    "    #new_min = MinPooling()(embed)\n",
    "    #new_min = Dropout(dropout_p)(new_min)\n",
    "    var = Dropout(dropout_p)(var)\n",
    "\n",
    "    new_ave = Dropout(dropout_p)(new_ave)\n",
    "    new_max = Dropout(dropout_p)(new_max)\n",
    "\n",
    "    topK_max = Dropout(dropout_p)(topK_max)\n",
    "\n",
    "    # con = new_max\n",
    "    # con = new_max\n",
    "    if chose==1:\n",
    "        con = concatenate([new_ave, new_max])\n",
    "    elif chose==2:\n",
    "        con = concatenate([new_ave, var])\n",
    "    output = Dense(64)(con)\n",
    "    output = Dropout(drop_out_p)(output)\n",
    "    main_output = Dense(num_class, activation='sigmoid')(output)\n",
    "    model = Model(inputs=main_input, outputs=main_output)\n",
    "    model.compile(loss=\"mse\", optimizer='adam', metrics=[auc_score, 'accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def auc_score(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成训练集的query与title并集与交集文件，我先生成了前1500W的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成训练集的并集与交集数据\n",
    "import time\n",
    "\n",
    "path = '/home/kesci/input/bytedance/first-round/train.csv'\n",
    "tra_data = pd.read_csv(\n",
    "        path,\n",
    "        header=None,\n",
    "        names=['query_id', 'query', 'query_title_id', 'title', 'label'],\n",
    "        iterator=True\n",
    "    )\n",
    "\n",
    "chunks = tra_data.get_chunk(15000000)\n",
    "# chunks.head()\n",
    "t1 = time.time()\n",
    "print('begin----')\n",
    "\n",
    "and_list = []  #交集\n",
    "diff_list = []  #差集\n",
    "for i in range(15000000):\n",
    "    if i%500000==0:\n",
    "        print(i, round(time.time() - t1, 4))\n",
    "    que_set = set(chunks['query'][i].split())\n",
    "    tit_set = set(chunks['title'][i].split())\n",
    "    and_list.append( ' '.join(list(que_set & tit_set)) )\n",
    "    diff_list.append( ' '.join(list(tit_set - que_set)) )\n",
    "print()\n",
    "print('生成并集与交集文件总共耗费了{}秒'.format( round(time.time() - t1, 4) ))\n",
    "print('and_list长度为{}'.format(len(and_list)))\n",
    "print('diff_list长度为{}'.format(len(diff_list)))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'andText':and_list,\n",
    "    'diffText':diff_list\n",
    "})\n",
    "print(df.shape)\n",
    "\n",
    "# 生成15M训练集并集差集文件\n",
    "df.to_csv('15M_and_diff_text.csv')\n",
    "print(time.time() - t1)\n",
    "# for ti in chunks['title'][:1]:\n",
    "#     print(ti)\n",
    "# chunks['title'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成测试集的并集与差集文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成测试集的交集和差集\n",
    "path = '/home/kesci/input/bytedance/first-round/test.csv'\n",
    "test_data = pd.read_csv(\n",
    "        path,\n",
    "        header=None,\n",
    "        names=['query_id', 'query', 'query_title_id', 'title']\n",
    "    )\n",
    "\n",
    "chunks = test_data\n",
    "chunks.head()\n",
    "t1 = time.time()\n",
    "print('begin----')\n",
    "and_list = []  #交集\n",
    "diff_list = []  #差集\n",
    "for i in range(len(chunks)):\n",
    "    if i%500000==0:\n",
    "        print(i, round(time.time() - t1, 4))\n",
    "    que_set = set(chunks['query'][i].split())\n",
    "    tit_set = set(chunks['title'][i].split())\n",
    "    and_list.append( ' '.join(list(que_set & tit_set)) )\n",
    "    diff_list.append( ' '.join(list(tit_set - que_set)) )\n",
    "print()\n",
    "\n",
    "print('总共耗费了{}秒'.format( round(time.time() - t1, 4) ))\n",
    "print('and_list长度为{}'.format(len(and_list)))\n",
    "print('diff_list长度为{}'.format(len(diff_list)))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'andText':and_list,\n",
    "    'diffText':diff_list\n",
    "})\n",
    "print(df.shape)\n",
    "df.to_csv('test_and_diff_text.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建词典，loop阈值确定了用多少训练集生成字典，例如：loop<15，代表使用1500W数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "path = \"/home/kesci/input/bytedance/first-round/\"\n",
    "ts = time.time()\n",
    "\n",
    "tra_data = pd.read_csv(\n",
    "                    path + \"train.csv\",\n",
    "                    header=None, \n",
    "                    names=['query_id', 'query', 'query_title_id', 'title', 'label'], \n",
    "                    iterator=True\n",
    "                    )\n",
    "\n",
    "loop = 0\n",
    "chunkSize = 1000000 \n",
    "min_count = 10  #最小频次\n",
    "voc_init = Counter([])\n",
    "voc_inv = [\"<PAD/>\"]  #padding词占位0\n",
    "# chunks = []\n",
    "nums = 0.\n",
    "# chunk = tra_data.get_chunk(chunkSize)\n",
    "# print(chunk)\n",
    "t_ = time.time()\n",
    "\n",
    "while loop < 15:\n",
    "    try:\n",
    "        loop += 1\n",
    "        # print(loop)\n",
    "        chunk = tra_data.get_chunk(chunkSize)\n",
    "        sentens = [s.split() for s in chunk['title']]\n",
    "        voc_init += Counter(itertools.chain(*sentens))\n",
    "        nums += chunk.shape[0]\n",
    "        # print('训练集加载{}秒,已经加载{}%'.format(time.time() - ts, 100*round(nums/100000000, 4)))\n",
    "        # chunks.append(chunk)\n",
    "    except StopIteration:\n",
    "        loop = 56\n",
    "        print(\"Iteration is stopped.\")\n",
    "        \n",
    "#构建词典\n",
    "del(chunk, sentens)\n",
    "\n",
    "voc_inv += [x[0] for x in voc_init.most_common() if x[1] >= min_count]\n",
    "    # Mapping from word to index\n",
    "voc_ = {x: i for i, x in enumerate(voc_inv)}\n",
    "print('build vocabulary over----, 用时 {} 秒'.format(round(time.time() - t_, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(voc_.keys())[:10])\n",
    "print(len(voc_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#参数设定和模型\n",
    "voc = voc_\n",
    "embedding_size = 100\n",
    "len_1 = 20\n",
    "len_2 = 25\n",
    "len_3 = 8\n",
    "len_4 = 15\n",
    "\n",
    "model = simple_4_input(\n",
    "    len_1,\n",
    "    len_2, \n",
    "    len_3, \n",
    "    len_4,\n",
    "    voc, \n",
    "    dropout_p=0.2,\n",
    "    num_class=1, \n",
    "    drop_out_p=0.2,\n",
    "    embedding_size=embedding_size)\n",
    "print('build model over ......')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid data\n",
    "valid_data = build_valid_4_input(voc, len_1, len_2, len_3, len_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify = model\n",
    "#使用了14W， samples_per_epoch * batch_size = 1400W，你们试的时候可以samples_per_epoch设置小一点看看\n",
    "classify.fit_generator(\n",
    "            generator_4_input(voc, len_1, len_2, len_3, len_4, batch_size=50000, samples=15000000),\n",
    "            samples_per_epoch=280, validation_data=valid_data,\n",
    "            epochs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成要提交的数据，也就是模型预测，提交的前两列 query_id和title_id  sub_info已经包含了\n",
    "sub, sub_info = build_test_4_input(voc, len_1, len_2, len_3, len_4)\n",
    "print('load over')\n",
    "y_sub = classify.predict(sub)\n",
    "print(y_sub.shape)\n",
    "print('predict over -----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成提交文件\n",
    "# print(y_sub.shape)\n",
    "sub_info['prediction'] = y_sub\n",
    "sub_info.head()\n",
    "sub_info.to_csv('AveMax_150embed_4_input_14M_v2.csv', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
