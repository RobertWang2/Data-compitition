{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    result=[]\n",
    "    for line in x:\n",
    "        result.append(np.exp(line)/np.sum(np.exp(line),axis=0))\n",
    "    return np.array(result)\n",
    "def compute_qauc(df,pred):\n",
    "    aucs=[]\n",
    "    df['pred']=pred\n",
    "    query_ids=df['query_id'].unique()\n",
    "    for query_id in query_ids:\n",
    "        tmp_df=df[df['query_id']==query_id]\n",
    "        label=tmp_df['label'].values\n",
    "        logit=tmp_df['pred'].values\n",
    "        if 0 in label and 1 in label:\n",
    "            auc=roc_auc_score(label,logit)\n",
    "        else:\n",
    "            auc=0.5\n",
    "        aucs.append(auc)\n",
    "    return np.mean(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size=128\n",
    "EPOCHS=10\n",
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self):\n",
    "        self.query=tf.placeholder(tf.int32,[None,20],name='query')\n",
    "        self.title=tf.placeholder(tf.int32,[None,30],name='title')\n",
    "        self.feature=tf.placeholder(tf.float32,[None,15],name='feature')\n",
    "        self.y=tf.placeholder(tf.int32,[None,],name='y')\n",
    "\n",
    "        with tf.variable_scope('EMB'):\n",
    "            emb=tf.get_variable('emb',shape=[len(vocab),emb_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "            query_emb=tf.nn.embedding_lookup(emb,self.query)\n",
    "            title_emb=tf.nn.embedding_lookup(emb,self.title)\n",
    "        with tf.variable_scope('GRU'):\n",
    "            bi_gru=Bidirectional(GRU(256,return_sequences=True))\n",
    "            query_gru=bi_gru(query_emb)\n",
    "            title_gru=bi_gru(title_emb)\n",
    "        with tf.variable_scope('Dense'):\n",
    "            feature_dense1=Dense(256,activation='relu')(self.feature)\n",
    "            feature=Dense(256,activation='relu')(feature_dense1)\n",
    "            \n",
    "            query_concat=tf.concat([query_emb,query_gru],axis=-1)\n",
    "            title_concat=tf.concat([title_emb,title_gru],axis=-1)\n",
    "            \n",
    "            query_dense=Dense(256,activation='relu')(query_concat)\n",
    "            title_dense=Dense(256,activation='relu')(title_concat)\n",
    "            \n",
    "            query_flatten=Flatten()(query_dense)\n",
    "            title_flatten=Flatten()(title_dense)\n",
    "        with tf.variable_scope('Concat'):\n",
    "            final_concat=tf.concat([query_flatten,title_flatten,feature],axis=-1)\n",
    "            final_batch=BatchNormalization()(final_concat)\n",
    "            final_dense=Dense(256,activation='relu')(final_batch)\n",
    "            self.logits=Dense(2)(final_dense)\n",
    "        self.loss=tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(labels=self.y,logits=self.logits))\n",
    "        self.train_step=tf.train.AdamOptimizer(0.0001).minimize(self.loss)\n",
    "        self.prediction = tf.to_int32(tf.argmax(self.logits, axis=-1))\n",
    "        self.correct_prediction=tf.cast(tf.equal(self.prediction, self.y), tf.float32)\n",
    "        self.acc = tf.reduce_mean(self.correct_prediction)\n",
    "        \n",
    "    def fit(self,query_train,title_train,feature_train,y_train,query_valid,title_valid,feature_valid,y_valid):\n",
    "        with tf.Session() as sess:\n",
    "            #sess.run(tf.global_variables_initializer())\n",
    "            saver=tf.train.Saver()\n",
    "            saver.restore(sess,'model//./model_epoch0.ckpt')\n",
    "            for epoch in range(1,EPOCHS):\n",
    "                start_time=time.time()\n",
    "                start=0\n",
    "                for step in range(len(query_train)//batch_size):\n",
    "                    feed_dict={\n",
    "                        self.query:query_train[start:start+batch_size],\n",
    "                        self.title:title_train[start:start+batch_size],\n",
    "                        self.feature:feature[start:start+batch_size],\n",
    "                        self.y:y_train[start:start+batch_size]\n",
    "                    }\n",
    "                   \n",
    "                    _,loss,acc=sess.run([self.train_step,self.loss,self.acc],feed_dict=feed_dict)\n",
    "                    if step%200==0:\n",
    "                        valid_dict={\n",
    "                            self.query:query_valid,\n",
    "                            self.title:title_valid,\n",
    "                            self.feature:feature_valid,\n",
    "                            self.y:y_valid                           \n",
    "                        }\n",
    "                        valid_loss,valid_acc,logits=sess.run([self.loss,self.acc,self.logits],feed_dict=valid_dict)\n",
    "                        logits=softmax(logits)\n",
    "                        qauc=compute_qauc(df_valid_final_1w_ql,logits[:,1])\n",
    "                        valid_auc=roc_auc_score(y_valid,logits[:,1])\n",
    "                        print('auc:',valid_auc)\n",
    "                        print('quc:',qauc)\n",
    "                        print('epoch {0} step {1} train_loss {2} train_acc {3} valid_loss:{4} valid_acc {5}'.format(epoch,step,loss,acc,valid_loss,valid_acc))\n",
    "                        \n",
    "                    if (step+1)%5000==0:\n",
    "                        saver.save(sess,'model//model_epoch{0}_step{1}.ckpt'.format(epoch,step+1))\n",
    "                    start+=batch_size\n",
    "                print(\"一个epoch用时{0}\".format(time.time()-start_time))\n",
    "                saver.save(sess,'model//model_epoch{0}.ckpt'.format(epoch))\n",
    "\n",
    "tf.reset_default_graph()\n",
    "model=Model()  \n",
    "model.fit(query_train,title_train,feature_train,y_train,query_valid,title_valid,feature_valid,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model=Model()\n",
    "with tf.Session() as sess:\n",
    "    saver=tf.train.Saver()\n",
    "    saver.restore(sess,'model//./model_epoch0.ckpt')\n",
    "    res=[]\n",
    "    for start in tqdm(np.arange(0,len(test_title_data),10000)):\n",
    "        test_feed_dict={\n",
    "            model.query:test_query_data[start:start+10000],\n",
    "            model.title:test_title_data[start:start+10000],\n",
    "            model.feature:feature_test[start:start+10000]\n",
    "        }\n",
    "        res.extend(softmax(np.squeeze(sess.run([model.logits],feed_dict=test_feed_dict))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
